day - 3

## Huffman Coding

### Definition:

Huffman Coding is a lossless data compression algorithm that assigns variable-length binary codes to characters, with more frequent characters getting shorter codes and less frequent characters getting longer codes. It builds an optimal binary tree (Huffman tree) where the path from root to each character determines its unique binary code, minimizing the total number of bits needed to represent the data.

**Key Properties:**

- Variable-length codes: Frequent characters = short codes, rare characters = long codes
- Prefix-free: No code is a prefix of another (ensures unique decoding)
- Optimal: Produces the minimum possible average code length
- Lossless: Perfect reconstruction of original data
- Greedy algorithm: Builds tree by repeatedly merging least frequent nodes

**Core Concepts:**

- Frequency analysis: Count how often each character appears
- Binary tree: Structure where each leaf is a character
- Priority queue: Used to build tree efficiently
- Bit encoding: Path from root to leaf gives character's code

### Example:

Compress the text "HELLO WORLD" efficiently

```
class HuffmanNode {
  constructor(char, freq, left = null, right = null) {
    this.char = char;
    this.freq = freq;
    this.left = left;
    this.right = right;
  }

  isLeaf() {
    return this.left === null && this.right === null;
  }
}

class HuffmanCoding {
  constructor(text) {
    this.text = text;
    this.frequencyMap = new Map();
    this.codes = new Map();
    this.tree = null;
  }

  // Step 1: Count character frequencies
  buildFrequencyMap() {
    console.log('üìä Building frequency map...');

    for (let char of this.text) {
      this.frequencyMap.set(char, (this.frequencyMap.get(char) || 0) + 1);
    }

    console.log('Character frequencies:');
    for (let [char, freq] of this.frequencyMap) {
      const displayChar = char === ' ' ? 'SPACE' : char;
      console.log(`  ${displayChar}: ${freq}`);
    }

    return this.frequencyMap;
  }

  // Step 2: Build Huffman tree using priority queue
  buildHuffmanTree() {
    console.log('\nüå≥ Building Huffman tree...');

    // Create priority queue (min-heap) with leaf nodes
    const priorityQueue = [];

    for (let [char, freq] of this.frequencyMap) {
      priorityQueue.push(new HuffmanNode(char, freq));
    }

    // Sort by frequency (ascending)
    priorityQueue.sort((a, b) => a.freq - b.freq);

    console.log('Initial nodes (sorted by frequency):');
    priorityQueue.forEach(node => {
      const displayChar = node.char === ' ' ? 'SPACE' : node.char;
      console.log(`  ${displayChar}: ${node.freq}`);
    });

    // Build tree by repeatedly merging two smallest nodes
    let step = 1;
    while (priorityQueue.length > 1) {
      console.log(`\nStep ${step}: Merging two smallest nodes`);

      // Remove two nodes with smallest frequency
      const left = priorityQueue.shift();
      const right = priorityQueue.shift();

      console.log(`  Merging: ${this.nodeToString(left)} + ${this.nodeToString(right)}`);

      // Create new internal node with combined frequency
      const merged = new HuffmanNode(
        null, // Internal nodes don't have characters
        left.freq + right.freq,
        left,
        right
      );

      console.log(`  Result: Internal node with frequency ${merged.freq}`);

      // Insert back into queue in correct position
      let inserted = false;
      for (let i = 0; i < priorityQueue.length; i++) {
        if (merged.freq <= priorityQueue[i].freq) {
          priorityQueue.splice(i, 0, merged);
          inserted = true;
          break;
        }
      }
      if (!inserted) {
        priorityQueue.push(merged);
      }

      step++;
    }

    this.tree = priorityQueue[0];
    console.log('\n‚úÖ Huffman tree built successfully!');
    return this.tree;
  }

  // Step 3: Generate codes by traversing tree
  generateCodes(node = this.tree, code = '') {
    if (!node) return;

    if (node.isLeaf()) {
      // Leaf node: assign code to character
      this.codes.set(node.char, code || '0'); // Handle single character case
    } else {
      // Internal node: traverse children
      this.generateCodes(node.left, code + '0');  // Left = 0
      this.generateCodes(node.right, code + '1'); // Right = 1
    }
  }

  // Step 4: Encode the text
  encode() {
    console.log('\nüî§ Generated Huffman codes:');

    this.generateCodes();

    // Display codes sorted by frequency (most frequent first)
    const sortedCodes = Array.from(this.codes.entries())
      .sort((a, b) => this.frequencyMap.get(b[0]) - this.frequencyMap.get(a[0]));

    for (let [char, code] of sortedCodes) {
      const displayChar = char === ' ' ? 'SPACE' : char;
      const freq = this.frequencyMap.get(char);
      console.log(`  ${displayChar}: ${code} (frequency: ${freq}, saves ${8 - code.length} bits per occurrence)`);
    }

    // Encode the text
    let encodedText = '';
    for (let char of this.text) {
      encodedText += this.codes.get(char);
    }

    console.log('\nüìù Encoding process:');
    console.log(`Original: "${this.text}"`);
    console.log('Character by character:');

    let currentEncoded = '';
    for (let char of this.text) {
      const code = this.codes.get(char);
      currentEncoded += code;
      const displayChar = char === ' ' ? 'SPACE' : char;
      console.log(`  ${displayChar} ‚Üí ${code}`);
    }

    console.log(`\nFinal encoded: ${encodedText}`);

    return encodedText;
  }

  // Step 5: Decode the binary string
  decode(encodedText) {
    console.log('\nüîì Decoding process:');
    console.log(`Encoded: ${encodedText}`);

    let decodedText = '';
    let currentNode = this.tree;

    console.log('Bit by bit decoding:');
    let bitIndex = 0;

    for (let bit of encodedText) {
      console.log(`  Bit ${bitIndex + 1}: ${bit}`);

      if (bit === '0') {
        currentNode = currentNode.left;
        console.log(`    ‚Üí Go left`);
      } else {
        currentNode = currentNode.right;
        console.log(`    ‚Üí Go right`);
      }

      if (currentNode.isLeaf()) {
        const char = currentNode.char;
        decodedText += char;
        const displayChar = char === ' ' ? 'SPACE' : char;
        console.log(`    ‚Üí Found character: ${displayChar}`);
        console.log(`    ‚Üí Reset to root for next character`);
        currentNode = this.tree; // Reset to root
      }

      bitIndex++;
    }

    console.log(`\nDecoded: "${decodedText}"`);
    return decodedText;
  }

  // Calculate compression statistics
  getCompressionStats() {
    const originalBits = this.text.length * 8; // ASCII = 8 bits per character

    let compressedBits = 0;
    for (let char of this.text) {
      compressedBits += this.codes.get(char).length;
    }

    const compressionRatio = ((originalBits - compressedBits) / originalBits * 100);

    return {
      originalSize: originalBits,
      compressedSize: compressedBits,
      savedBits: originalBits - compressedBits,
      compressionRatio: compressionRatio.toFixed(1)
    };
  }

  // Helper method for displaying nodes
  nodeToString(node) {
    if (node.isLeaf()) {
      const displayChar = node.char === ' ' ? 'SPACE' : node.char;
      return `${displayChar}(${node.freq})`;
    } else {
      return `Internal(${node.freq})`;
    }
  }

  // Visualize the tree structure
  visualizeTree(node = this.tree, prefix = '', isLast = true) {
    if (!node) return;

    console.log(prefix + (isLast ? '‚îî‚îÄ‚îÄ ' : '‚îú‚îÄ‚îÄ ') + this.nodeToString(node));

    if (!node.isLeaf()) {
      const newPrefix = prefix + (isLast ? '    ' : '‚îÇ   ');
      this.visualizeTree(node.left, newPrefix, false);
      this.visualizeTree(node.right, newPrefix, true);
    }
  }
}

// Demonstration
function demonstrateHuffmanCoding() {
  const text = "HELLO WORLD";
  console.log('üóúÔ∏è HUFFMAN CODING DEMONSTRATION');
  console.log('===============================');
  console.log(`Input text: "${text}"`);
  console.log(`Text length: ${text.length} characters\n`);

  const huffman = new HuffmanCoding(text);

  // Step 1: Build frequency map
  huffman.buildFrequencyMap();

  // Step 2: Build Huffman tree
  huffman.buildHuffmanTree();

  // Visualize the tree
  console.log('\nüå≥ Huffman Tree Structure:');
  huffman.visualizeTree();

  // Step 3 & 4: Generate codes and encode
  const encoded = huffman.encode();

  // Step 5: Decode to verify
  const decoded = huffman.decode(encoded);

  // Statistics
  console.log('\nüìà Compression Statistics:');
  const stats = huffman.getCompressionStats();
  console.log(`Original size: ${stats.originalSize} bits (${text.length} √ó 8 bits/char)`);
  console.log(`Compressed size: ${stats.compressedSize} bits`);
  console.log(`Space saved: ${stats.savedBits} bits`);
  console.log(`Compression ratio: ${stats.compressionRatio}%`);

  // Verification
  console.log('\n‚úÖ Verification:');
  console.log(`Original:  "${text}"`);
  console.log(`Decoded:   "${decoded}"`);
  console.log(`Match: ${text === decoded ? '‚úÖ Perfect!' : '‚ùå Error!'}`);
}

demonstrateHuffmanCoding();
```

---

day - 4

## JIT(Just In Time) Compiler

### Definition:

JIT Compiler is a compilation technique that compiles code during runtime rather than beforehand. It translates high-level code (like Java bytecode or JavaScript) into native machine code at the moment it's needed for execution. JIT compilers analyze running code to identify frequently executed "hot spots" and optimize them aggressively, providing a balance between compilation speed and execution performance.

**Key Properties:**

- Runtime compilation: Code is compiled while the program runs
- Adaptive optimization: Optimizes based on actual usage patterns
- Hot spot detection: Focuses optimization on frequently executed code
- Profile-guided: Uses runtime profiling to make optimization decisions
- Dynamic: Can recompile and re-optimize code as patterns change

**Core Concepts:**

- Interpretation: Initial execution without compilation
- Compilation threshold: When code becomes "hot enough" to compile
- Optimization levels: Multiple tiers of increasingly aggressive optimization
- Deoptimization: Fall back when assumptions prove wrong
- Code cache: Store compiled machine code for reuse

### Example:

JavaScript V8 Engine

```
// JavaScript JIT optimization example
function hotFunction(arr) {
    let sum = 0;
    // V8 will optimize this loop after many iterations
    for (let i = 0; i < arr.length; i++) {
        sum += arr[i];  // Type specialization: assumes numbers
    }
    return sum;
}

// Phase 1: Interpretation (Ignition interpreter)
for (let i = 0; i < 100; i++) {
    hotFunction([1, 2, 3, 4, 5]);
}

// Phase 2: Basic compilation (TurboFan compiler)
for (let i = 0; i < 10000; i++) {
    hotFunction([1, 2, 3, 4, 5]);  // Now compiled with type assumptions
}

// Phase 3: Deoptimization if assumptions break
hotFunction([1, 2, 3.14, "string"]);  // Mixed types ‚Üí deoptimize back to interpreter
```

---

day - 5

## Sandbox Architecture

### Definition:

Sandbox Architecture is a security design pattern that creates isolated execution environments where untrusted code can run safely without affecting the host system or other applications. Like a child's sandbox that contains sand and prevents it from spreading everywhere, a security sandbox contains potentially dangerous code and prevents it from accessing sensitive resources, making unauthorized network calls, or damaging the system.

**Key Properties:**

- Isolation: Code runs in a restricted environment separate from the host
- Controlled access: Limited permissions to system resources
- Containment: Prevents malicious code from escaping or causing damage
- Monitoring: All activities within the sandbox are observable
- Disposable: Sandbox can be destroyed and recreated easily

**Core Concepts:**

- Security boundaries: Clear separation between trusted and untrusted code
- Privilege restriction: Minimal permissions principle
- Resource limitations: CPU, memory, disk, and network constraints
- API filtering: Only allowed operations can be performed
- Process isolation: Separate processes or containers for untrusted code

### Example:

Allow users to upload and run custom JavaScript code on your website without compromising security

```
// Chrome's multi-process architecture
const browserSandbox = {
  browserProcess: {
    privileges: 'FULL',
    responsibilities: ['UI', 'Network', 'File System', 'Process Management']
  },

  rendererProcess: {
    privileges: 'RESTRICTED',
    responsibilities: ['Render Web Pages', 'Execute JavaScript'],
    restrictions: [
      'No direct file system access',
      'No direct network access',
      'No system API access',
      'Must communicate via IPC'
    ]
  },

  pluginProcess: {
    privileges: 'MINIMAL',
    responsibilities: ['Run Browser Plugins'],
    restrictions: [
      'Heavily restricted API access',
      'Limited memory allocation',
      'Network access via browser process only'
    ]
  }
};
```

---

day - 6

## Conway's Law

### Definition:

Conway's Law states that "organizations which design systems... are constrained to produce designs which are copies of the communication structures of these organizations." In simple terms, the software architecture you build will mirror how your teams are organized and communicate. If your teams are siloed, your software will be siloed; if your teams collaborate well, your software will be well-integrated.

**Key Properties:**

- Organizational mirror: Software structure reflects team structure
- Communication patterns: How teams talk affects how code is structured
- Inevitable influence: Happens whether you plan for it or not
- Structural constraint: Team boundaries become system boundaries
- Feedback loop: System structure can reinforce organizational structure

**Core Concepts:**

- Team boundaries: Separate teams create separate system components
- Communication overhead: Cross-team coordination creates system coupling
- Ownership domains: Teams own specific parts of the system
- Interface design: Team interactions define system interfaces
- Reverse Conway maneuver: Reorganize teams to get desired architecture

### Example:

cross-functional feature teams

```
üöÄ CROSS-FUNCTIONAL FEATURE TEAMS
=================================

User Management Team (5 people)
‚îú‚îÄ‚îÄ 1 Frontend Developer
‚îú‚îÄ‚îÄ 1 Backend Developer
‚îú‚îÄ‚îÄ 1 Mobile Developer
‚îú‚îÄ‚îÄ 1 Database Expert
‚îî‚îÄ‚îÄ 1 DevOps Engineer
‚îî‚îÄ‚îÄ Owns: User registration, authentication, profiles

Product Catalog Team (5 people)
‚îú‚îÄ‚îÄ 1 Frontend Developer
‚îú‚îÄ‚îÄ 1 Backend Developer
‚îú‚îÄ‚îÄ 1 Mobile Developer
‚îú‚îÄ‚îÄ 1 Database Expert
‚îî‚îÄ‚îÄ 1 DevOps Engineer
‚îî‚îÄ‚îÄ Owns: Product display, search, recommendations

Order Processing Team (5 people)
‚îú‚îÄ‚îÄ 1 Frontend Developer
‚îú‚îÄ‚îÄ 1 Backend Developer
‚îú‚îÄ‚îÄ 1 Mobile Developer
‚îú‚îÄ‚îÄ 1 Database Expert
‚îî‚îÄ‚îÄ 1 DevOps Engineer
‚îî‚îÄ‚îÄ Owns: Shopping cart, checkout, order management

Payment Team (5 people)
‚îú‚îÄ‚îÄ 1 Frontend Developer
‚îú‚îÄ‚îÄ 1 Backend Developer
‚îú‚îÄ‚îÄ 1 Mobile Developer
‚îú‚îÄ‚îÄ 1 Database Expert
‚îî‚îÄ‚îÄ 1 DevOps Engineer
‚îî‚îÄ‚îÄ Owns: Payment processing, billing, refunds
```

---

day - 7

## Core Event-Driven Architecture (EDA) Patterns

### Definition:

Event-Driven Architecture (EDA) is a design pattern where components communicate through the production and consumption of events. Instead of direct service-to-service calls, systems emit events when something significant happens, and interested parties subscribe to and react to these events. This creates loosely coupled, scalable systems where components don't need to know about each other directly.

**Key Properties:**

- Asynchronous communication: Events are processed without blocking the producer
- Loose coupling: Producers and consumers don't directly depend on each other
- Event-first design: Business logic is expressed as events and reactions
- Temporal decoupling: Producers and consumers don't need to be online simultaneously
- Scalable: Easy to add new event consumers without changing producers

**Core Concepts:**

- Events: Immutable facts about what happened in the system
- Event producers: Components that emit events
- Event consumers: Components that react to events
- Event bus/broker: Infrastructure that routes events
- Event store: Persistent storage for events

### Example:

Netflix

```
Event: "User clicked play on Movie X"
Reactions:
‚Ä¢ Billing Service: Check subscription status
‚Ä¢ Recommendation Service: Update viewing patterns
‚Ä¢ Analytics Service: Track engagement metrics
‚Ä¢ CDN Service: Pre-cache related content
‚Ä¢ UI Service: Update "Continue Watching" list
```

---

day - 10

## PKCE(Proof Key for Code Exchange)

### Definition:

PKCE (Proof Key for Code Exchange) is a security extension to OAuth 2.0 designed to make the authorization code flow secure for public clients (like mobile apps and SPAs) that cannot securely store client secrets. Instead of using a static client secret, PKCE dynamically generates a cryptographic challenge and verifier pair for each authorization request, preventing authorization code interception attacks.

**Key Properties:**

- Dynamic security: Each authorization request uses unique cryptographic values
- No client secret required: Suitable for public clients (mobile apps, SPAs)
- Prevents code interception: Even if auth code is stolen, it's useless without the verifier
- Cryptographically secure: Uses SHA256 hashing with random code verifiers
- Backward compatible: Works with existing OAuth 2.0 infrastructure

**Core Concepts:**

- Code Verifier: Random cryptographic string (43-128 characters)
- Code Challenge: SHA256 hash of the code verifier (Base64 URL-encoded)
- Challenge Method: How the challenge is derived (S256 for SHA256)
- Authorization code: Still used, but protected by PKCE
- Proof of possession: Client must prove it initiated the flow

### Example:

Mobile app needs to authenticate users securely without storing secrets

```
// ‚úÖ PKCE-PROTECTED OAUTH 2.0 FLOW
class SecurePKCEFlow {
  constructor() {
    this.crypto = require('crypto'); // In browser, use Web Crypto API
  }

  async demonstrateSecureFlow() {
    console.log('\nüîí SECURE OAUTH 2.0 WITH PKCE');
    console.log('=============================');
    console.log('Solution: Cryptographic proof that client initiated the flow\n');

    // Step 1: Generate PKCE values
    console.log('1Ô∏è‚É£ Mobile app generates PKCE values:');
    const pkcePair = this.generatePKCEPair();

    console.log(`   Code Verifier: ${pkcePair.codeVerifier.substring(0, 20)}... (${pkcePair.codeVerifier.length} chars)`);
    console.log(`   Code Challenge: ${pkcePair.codeChallenge}`);
    console.log(`   Challenge Method: ${pkcePair.challengeMethod}`);
    console.log('   ‚úÖ Verifier stored securely in app memory');

    // Step 2: Authorization request with code challenge
    const authUrl = this.buildPKCEAuthorizationUrl(pkcePair.codeChallenge, pkcePair.challengeMethod);
    console.log('\n2Ô∏è‚É£ Mobile app redirects user with PKCE challenge:');
    console.log(`   ${authUrl}`);
    console.log('   ‚úÖ Code challenge sent, verifier stays in app');

    // Step 3: User authorizes, gets code
    const authorizationCode = 'auth_code_67890';
    console.log('\n3Ô∏è‚É£ User authorizes, gets redirected back:');
    console.log(`   myapp://callback?code=${authorizationCode}`);
    console.log('   ‚ö†Ô∏è  Code can still be intercepted...');

    // Step 4: Legitimate app exchanges code with verifier
    console.log('\n4Ô∏è‚É£ Legitimate app exchanges code with PKCE verifier:');
    const tokenResponse = await this.exchangeCodeForToken(authorizationCode, pkcePair.codeVerifier);
    console.log('   POST /token');
    console.log('   {');
    console.log('     "grant_type": "authorization_code",');
    console.log(`     "code": "${authorizationCode}",`);
    console.log('     "client_id": "mobile_app_123",');
    console.log(`     "code_verifier": "${pkcePair.codeVerifier.substring(0, 20)}..."`);
    console.log('   }');

    if (tokenResponse.success) {
      console.log('\n‚úÖ SUCCESS: Token exchange successful!');
      console.log(`   Access Token: ${tokenResponse.accessToken.substring(0, 20)}...`);
    }

    // Step 5: Attack scenario with PKCE protection
    console.log('\nüíÄ ATTACK ATTEMPT: Attacker tries to use intercepted code:');
    console.log('   Attacker has: authorization code');
    console.log('   Attacker missing: code verifier (only in legitimate app memory)');

    const attackResponse = await this.attemptAttack(authorizationCode);
    console.log('\n‚ùå ATTACK FAILED:');
    console.log(`   Error: ${attackResponse.error}`);
    console.log('   üõ°Ô∏è PKCE protection worked!');
  }

  generatePKCEPair() {
    // Generate random code verifier (43-128 characters)
    const codeVerifier = this.generateCodeVerifier();

    // Generate code challenge (SHA256 hash of verifier)
    const codeChallenge = this.generateCodeChallenge(codeVerifier);

    return {
      codeVerifier,
      codeChallenge,
      challengeMethod: 'S256'
    };
  }

  generateCodeVerifier() {
    // Generate cryptographically random string (43-128 chars, URL-safe)
    const buffer = this.crypto.randomBytes(32); // 32 bytes = 43 chars when base64url encoded
    return this.base64URLEncode(buffer);
  }

  generateCodeChallenge(codeVerifier) {
    // SHA256 hash of code verifier, then base64url encode
    const hash = this.crypto.createHash('sha256').update(codeVerifier).digest();
    return this.base64URLEncode(hash);
  }

  base64URLEncode(buffer) {
    return buffer.toString('base64')
      .replace(/\+/g, '-')
      .replace(/\//g, '_')
      .replace(/=/g, ''); // Remove padding
  }

  buildPKCEAuthorizationUrl(codeChallenge, challengeMethod) {
    const params = new URLSearchParams({
      response_type: 'code',
      client_id: 'mobile_app_123',
      redirect_uri: 'myapp://callback',
      scope: 'read_profile read_emails',
      state: 'random_state_value',
      code_challenge: codeChallenge,
      code_challenge_method: challengeMethod
    });

    return `https://auth.example.com/oauth/authorize?${params}`;
  }

  async exchangeCodeForToken(authCode, codeVerifier) {
    // Simulate token exchange request
    console.log('   üîç Authorization server validates:');
    console.log('     1. Code is valid and not expired');
    console.log('     2. SHA256(code_verifier) === stored code_challenge');
    console.log('     3. Client_id matches original request');

    // Simulate server-side PKCE verification
    const isValidPKCE = this.verifyPKCE(codeVerifier, 'stored_code_challenge_from_auth_request');

    if (isValidPKCE) {
      return {
        success: true,
        accessToken: 'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...',
        tokenType: 'Bearer',
        expiresIn: 3600,
        refreshToken: 'refresh_token_abc123',
        scope: 'read_profile read_emails'
      };
    } else {
      return {
        success: false,
        error: 'invalid_grant',
        errorDescription: 'PKCE verification failed'
      };
    }
  }

  async attemptAttack(authCode) {
    console.log('   POST /token (attacker without code_verifier):');
    console.log('   {');
    console.log('     "grant_type": "authorization_code",');
    console.log(`     "code": "${authCode}",`);
    console.log('     "client_id": "mobile_app_123"');
    console.log('     // Missing code_verifier!');
    console.log('   }');

    // Attacker doesn't have the code verifier
    return {
      error: 'invalid_request',
      errorDescription: 'Code verifier required for PKCE'
    };
  }

  verifyPKCE(codeVerifier, storedCodeChallenge) {
    // In real implementation, server would compare like this:
    const computedChallenge = this.generateCodeChallenge(codeVerifier);
    return computedChallenge === storedCodeChallenge;
  }
}
```

---

day - 11

## Apache Avro Serialization

### Definition:

Apache Avro is a data serialization framework that provides compact, fast, and language-neutral data exchange. It uses JSON-based schemas to define data structures and serializes data into a compact binary format. Unlike other serialization formats, Avro includes the schema with the data or provides schema evolution capabilities, making it ideal for systems where data structures change over time while maintaining backward/forward compatibility.

**Key Properties:**

- Schema-based: Data structure defined in JSON schemas
- Compact binary format: Efficient storage and transmission
- Schema evolution: Add, remove, or modify fields while maintaining compatibility
- Language-neutral: Works across different programming languages
- Self-describing: Schema can be embedded with data or referenced separately
- Dynamic typing: Can handle data without compile-time schema knowledge

**Core Concepts:**

- Schema: JSON definition of data structure and types
- Schema registry: Central repository for schema management
- Schema evolution: Rules for compatible schema changes
- Serialization: Convert objects to binary format using schema
- Deserialization: Convert binary data back to objects
- Schema fingerprinting: Unique identification of schema versions

### Example:

Apache Kafka + Avro:

```
Event Streaming Pipeline:
User Action ‚Üí Kafka (Avro) ‚Üí Stream Processing ‚Üí Analytics DB

Benefits:
‚Ä¢ Schema registry ensures data quality
‚Ä¢ Backward/forward compatibility
‚Ä¢ Compact message size
‚Ä¢ Strong typing validation
```

---

day - 12

## Modular Monoliths

### Definition:

Modular Monoliths are single deployable applications that are internally organized into well-defined, loosely coupled modules with clear boundaries and interfaces. Unlike traditional monoliths where code is tightly coupled, modular monoliths enforce module separation through architectural patterns while maintaining the operational simplicity of a single deployment unit. They provide the development benefits of microservices (clear boundaries, team ownership) without the operational complexity (distributed systems, network calls, deployment coordination).

**Key Properties:**

- Single deployment unit: One application with multiple internal modules
- Clear module boundaries: Well-defined interfaces between modules
- Loose internal coupling: Modules communicate through defined contracts
- High module cohesion: Related functionality grouped together
- Enforceable separation: Architecture prevents unwanted dependencies
- Team ownership: Different teams can own different modules

**Core Concepts:**

- Modules: Self-contained business domains with clear responsibilities
- Module interfaces: Public APIs that define how modules communicate
- Dependency inversion: Modules depend on abstractions, not implementations
- Shared kernel: Common infrastructure and utilities
- Module boundaries: Logical separation enforced by architecture
- Domain-driven design: Modules aligned with business domains

### Example:

E-commerce platform needs clear separation of concerns but wants to avoid microservices complexity

```
// ‚úÖ MODULAR MONOLITH - WELL-DEFINED MODULE BOUNDARIES
class ModularEcommerceMonolith {
  constructor() {
    console.log('‚úÖ MODULAR MONOLITH ARCHITECTURE');
    console.log('================================');

    this.setupModules();
    this.demonstrateModularStructure();
  }

  setupModules() {
    // Each module has clear boundaries and interfaces
    this.modules = {
      userManagement: new UserManagementModule(),
      productCatalog: new ProductCatalogModule(),
      orderProcessing: new OrderProcessingModule(),
      paymentProcessing: new PaymentProcessingModule(),
      notification: new NotificationModule(),
      shared: new SharedModule()
    };

    // Wire up module dependencies through interfaces
    this.wireModules();
  }

  wireModules() {
    console.log('\nüîå Wiring Module Dependencies:');

    // Order module depends on interfaces, not implementations
    this.modules.orderProcessing.setUserService(this.modules.userManagement);
    this.modules.orderProcessing.setProductService(this.modules.productCatalog);
    this.modules.orderProcessing.setPaymentService(this.modules.paymentProcessing);
    this.modules.orderProcessing.setNotificationService(this.modules.notification);

    console.log('‚úÖ All modules wired through defined interfaces');
  }

  demonstrateModularStructure() {
    console.log('\nüìÅ Modular Structure:');
    console.log('/src');
    console.log('  /modules');
    console.log('    /user-management');
    console.log('      /api          ‚Üê Public interface');
    console.log('        - UserService.js');
    console.log('        - IUserRepository.js');
    console.log('      /domain       ‚Üê Business logic');
    console.log('        - User.js');
    console.log('        - UserValidator.js');
    console.log('      /infrastructure ‚Üê Implementation details');
    console.log('        - UserRepository.js');
    console.log('        - UserController.js');
    console.log('');
    console.log('    /product-catalog');
    console.log('      /api');
    console.log('        - ProductService.js');
    console.log('        - IProductRepository.js');
    console.log('      /domain');
    console.log('        - Product.js');
    console.log('        - Inventory.js');
    console.log('      /infrastructure');
    console.log('        - ProductRepository.js');
    console.log('        - ProductController.js');
    console.log('');
    console.log('    /order-processing');
    console.log('      /api');
    console.log('        - OrderService.js');
    console.log('      /domain');
    console.log('        - Order.js');
    console.log('        - OrderWorkflow.js');
    console.log('      /infrastructure');
    console.log('        - OrderRepository.js');
    console.log('        - OrderController.js');
    console.log('');
    console.log('  /shared');
    console.log('    /common');
    console.log('      - Database.js');
    console.log('      - Logger.js');
    console.log('      - Events.js');

    console.log('\nüéØ Module Principles:');
    console.log('‚úÖ Each module has single responsibility');
    console.log('‚úÖ Modules communicate through defined interfaces');
    console.log('‚úÖ Internal implementation is hidden');
    console.log('‚úÖ Clear team ownership per module');
    console.log('‚úÖ Dependencies point inward (dependency inversion)');
  }

  async processOrder(orderData) {
    console.log('\nüõí MODULAR ORDER PROCESSING EXAMPLE');
    console.log('===================================');

    try {
      // Order module orchestrates through interfaces only
      const result = await this.modules.orderProcessing.processOrder(orderData);

      console.log('‚úÖ Order processed successfully!');
      console.log(`Order ID: ${result.orderId}`);

      return result;

    } catch (error) {
      console.log(`‚ùå Order processing failed: ${error.message}`);
      throw error;
    }
  }

  demonstrateModuleBenefits() {
    console.log('\nüéâ MODULAR MONOLITH BENEFITS');
    console.log('============================');

    console.log('üèóÔ∏è Architecture Benefits:');
    console.log('‚Ä¢ Clear module boundaries prevent architectural drift');
    console.log('‚Ä¢ Enforced separation through interfaces');
    console.log('‚Ä¢ Easy to understand and navigate');
    console.log('‚Ä¢ Testable modules in isolation');

    console.log('\nüë• Team Benefits:');
    console.log('‚Ä¢ Each team owns specific modules');
    console.log('‚Ä¢ Teams can work independently');
    console.log('‚Ä¢ Clear contracts between teams');
    console.log('‚Ä¢ Reduced coordination overhead');

    console.log('\nüöÄ Operational Benefits:');
    console.log('‚Ä¢ Single deployment unit');
    console.log('‚Ä¢ No network latency between modules');
    console.log('‚Ä¢ Shared infrastructure and monitoring');
    console.log('‚Ä¢ ACID transactions across modules');
    console.log('‚Ä¢ Simpler debugging and testing');

    console.log('\nüîÑ Evolution Benefits:');
    console.log('‚Ä¢ Can extract modules to microservices later');
    console.log('‚Ä¢ Modules evolve independently');
    console.log('‚Ä¢ Easy to refactor within module boundaries');
    console.log('‚Ä¢ Clear migration path if needed');
  }
}

// Module Implementations
class UserManagementModule {
  constructor() {
    this.repository = new UserRepository();
    console.log('üìã User Management Module initialized');
  }

  // Public interface - what other modules can use
  async getUser(userId) {
    console.log(`üîç UserModule: Getting user ${userId}`);
    return await this.repository.findById(userId);
  }

  async validateUser(userId) {
    console.log(`‚úÖ UserModule: Validating user ${userId}`);
    const user = await this.getUser(userId);
    return user && user.status === 'active';
  }

  async getUserPreferences(userId) {
    console.log(`‚öôÔ∏è UserModule: Getting preferences for user ${userId}`);
    const user = await this.getUser(userId);
    return user ? user.preferences : null;
  }

  // Internal methods - not exposed to other modules
  async updateLastLogin(userId) {
    console.log(`üìÖ UserModule: Updating last login for ${userId}`);
    return await this.repository.updateLastLogin(userId);
  }
}

class ProductCatalogModule {
  constructor() {
    this.repository = new ProductRepository();
    console.log('üõçÔ∏è Product Catalog Module initialized');
  }

  // Public interface
  async getProduct(productId) {
    console.log(`üì¶ ProductModule: Getting product ${productId}`);
    return await this.repository.findById(productId);
  }

  async checkAvailability(productId, quantity) {
    console.log(`üìä ProductModule: Checking availability for ${productId} (qty: ${quantity})`);
    const product = await this.getProduct(productId);
    return product && product.stock >= quantity;
  }

  async reserveProducts(items) {
    console.log(`üîí ProductModule: Reserving ${items.length} items`);

    for (const item of items) {
      const available = await this.checkAvailability(item.productId, item.quantity);
      if (!available) {
        throw new Error(`Product ${item.productId} not available`);
      }
    }

    // Reserve inventory
    const reservation = {
      reservationId: `res_${Date.now()}`,
      items,
      expiresAt: new Date(Date.now() + 15 * 60 * 1000) // 15 minutes
    };

    console.log(`‚úÖ ProductModule: Reserved items with ID ${reservation.reservationId}`);
    return reservation;
  }
}

class OrderProcessingModule {
  constructor() {
    this.repository = new OrderRepository();
    this.userService = null;
    this.productService = null;
    this.paymentService = null;
    this.notificationService = null;
    console.log('üìã Order Processing Module initialized');
  }

  // Dependency injection - modules depend on interfaces
  setUserService(userService) {
    this.userService = userService;
  }

  setProductService(productService) {
    this.productService = productService;
  }

  setPaymentService(paymentService) {
    this.paymentService = paymentService;
  }

  setNotificationService(notificationService) {
    this.notificationService = notificationService;
  }

  async processOrder(orderData) {
    console.log('\nüìã OrderModule: Starting order processing workflow...');

    // Step 1: Validate user
    console.log('1Ô∏è‚É£ Validating user...');
    const userValid = await this.userService.validateUser(orderData.userId);
    if (!userValid) {
      throw new Error('Invalid user');
    }
    console.log('   ‚úÖ User validated');

    // Step 2: Reserve products
    console.log('2Ô∏è‚É£ Reserving products...');
    const reservation = await this.productService.reserveProducts(orderData.items);
    console.log(`   ‚úÖ Products reserved: ${reservation.reservationId}`);

    // Step 3: Create order
    console.log('3Ô∏è‚É£ Creating order record...');
    const order = {
      orderId: `order_${Date.now()}`,
      userId: orderData.userId,
      items: orderData.items,
      total: orderData.total,
      status: 'created',
      reservationId: reservation.reservationId,
      createdAt: new Date()
    };

    await this.repository.save(order);
    console.log(`   ‚úÖ Order created: ${order.orderId}`);

    // Step 4: Process payment
    console.log('4Ô∏è‚É£ Processing payment...');
    const paymentResult = await this.paymentService.processPayment({
      orderId: order.orderId,
      amount: order.total,
      paymentMethod: orderData.paymentMethod
    });

    if (!paymentResult.success) {
      console.log('   ‚ùå Payment failed, cancelling order...');
      await this.cancelOrder(order.orderId);
      throw new Error('Payment processing failed');
    }

    console.log(`   ‚úÖ Payment processed: ${paymentResult.transactionId}`);

    // Step 5: Update order status
    console.log('5Ô∏è‚É£ Finalizing order...');
    order.status = 'confirmed';
    order.paymentId = paymentResult.transactionId;
    await this.repository.save(order);

    // Step 6: Send notifications
    console.log('6Ô∏è‚É£ Sending notifications...');
    await this.notificationService.sendOrderConfirmation(order.userId, order);

    console.log('   ‚úÖ Order processing completed!');
    return order;
  }

  async cancelOrder(orderId) {
    console.log(`‚ùå OrderModule: Cancelling order ${orderId}`);
    // Implementation for order cancellation
  }
}

class PaymentProcessingModule {
  constructor() {
    console.log('üí≥ Payment Processing Module initialized');
  }

  async processPayment(paymentData) {
    console.log(`üí≥ PaymentModule: Processing payment for order ${paymentData.orderId}`);
    console.log(`   Amount: $${paymentData.amount}`);
    console.log(`   Method: ${paymentData.paymentMethod}`);

    // Simulate payment processing
    await this.delay(100);

    // 90% success rate for demo
    const success = Math.random() > 0.1;

    if (success) {
      const result = {
        success: true,
        transactionId: `txn_${Date.now()}`,
        amount: paymentData.amount,
        processedAt: new Date()
      };

      console.log(`   ‚úÖ Payment successful: ${result.transactionId}`);
      return result;
    } else {
      console.log('   ‚ùå Payment failed: Card declined');
      return { success: false, error: 'Card declined' };
    }
  }

  delay(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}

class NotificationModule {
  constructor() {
    console.log('üìß Notification Module initialized');
  }

  async sendOrderConfirmation(userId, order) {
    console.log(`üìß NotificationModule: Sending confirmation for order ${order.orderId}`);
    console.log(`   To: User ${userId}`);
    console.log(`   Subject: Order Confirmation - ${order.orderId}`);
    console.log(`   ‚úÖ Email sent successfully`);
  }

  async sendShippingNotification(userId, trackingInfo) {
    console.log(`üì¶ NotificationModule: Sending shipping notification`);
    console.log(`   Tracking: ${trackingInfo.trackingNumber}`);
  }
}

class SharedModule {
  constructor() {
    this.database = new Database();
    this.logger = new Logger();
    this.eventBus = new EventBus();
    console.log('üîß Shared Module initialized');
  }

  getDatabase() {
    return this.database;
  }

  getLogger() {
    return this.logger;
  }

  getEventBus() {
    return this.eventBus;
  }
}

// Infrastructure implementations
class UserRepository {
  async findById(userId) {
    return {
      id: userId,
      name: 'John Doe',
      email: 'john@example.com',
      status: 'active',
      preferences: { notifications: true, language: 'en' }
    };
  }

  async updateLastLogin(userId) {
    console.log(`üìù Database: Updated last login for user ${userId}`);
  }
}

class ProductRepository {
  async findById(productId) {
    return {
      id: productId,
      name: `Product ${productId}`,
      price: 99.99,
      stock: 50
    };
  }
}

class OrderRepository {
  constructor() {
    this.orders = new Map();
  }

  async save(order) {
    this.orders.set(order.orderId, order);
    console.log(`üìù Database: Saved order ${order.orderId}`);
  }

  async findById(orderId) {
    return this.orders.get(orderId);
  }
}

class Database {
  constructor() {
    console.log('üóÑÔ∏è Database connection established');
  }
}

class Logger {
  constructor() {
    console.log('üìù Logger initialized');
  }
}

class EventBus {
  constructor() {
    console.log('üì° Event bus initialized');
  }
}

// Advanced Modular Monolith Patterns
class AdvancedModularPatterns {
  demonstrateModuleEvolution() {
    console.log('\nüîÑ MODULE EVOLUTION STRATEGIES');
    console.log('==============================');

    console.log('üìà Evolution Path:');
    console.log('1. Start: Traditional Monolith');
    console.log('   ‚Üì (Refactor to modules)');
    console.log('2. Modular Monolith');
    console.log('   ‚Üì (Extract high-value modules)');
    console.log('3. Hybrid: Some microservices + modular monolith');
    console.log('   ‚Üì (Continue selective extraction)');
    console.log('4. Full Microservices (if needed)');

    console.log('\nüéØ When to Extract a Module:');
    console.log('‚úÖ Different scaling requirements');
    console.log('‚úÖ Different team/technology preferences');
    console.log('‚úÖ Regulatory/security boundaries');
    console.log('‚úÖ Performance bottlenecks');
    console.log('‚úÖ High-value business domain');

    console.log('\n‚ùå Don\'t Extract If:');
    console.log('‚Ä¢ Tight coupling with other modules');
    console.log('‚Ä¢ Low change frequency');
    console.log('‚Ä¢ Complex cross-module transactions');
    console.log('‚Ä¢ Small team that owns multiple modules');
  }

  showModularPatterns() {
    console.log('\nüèóÔ∏è ADVANCED MODULAR PATTERNS');
    console.log('============================');

    console.log('üîå Dependency Inversion:');
    console.log('‚Ä¢ Modules depend on interfaces, not implementations');
    console.log('‚Ä¢ High-level modules don\'t depend on low-level modules');
    console.log('‚Ä¢ Both depend on abstractions');

    console.log('\nüì° Event-Driven Communication:');
    console.log('‚Ä¢ Modules publish domain events');
    console.log('‚Ä¢ Other modules subscribe to relevant events');
    console.log('‚Ä¢ Reduces coupling between modules');

    console.log('\nüõ°Ô∏è Module Boundaries Enforcement:');
    console.log('‚Ä¢ Architecture tests validate boundaries');
    console.log('‚Ä¢ Package visibility rules');
    console.log('‚Ä¢ Build-time dependency checking');

    console.log('\nüîÑ Shared Kernel:');
    console.log('‚Ä¢ Common utilities and infrastructure');
    console.log('‚Ä¢ Carefully managed shared code');
    console.log('‚Ä¢ Versioned and backward compatible');
  }
}
```

---

day - 13

## PostgreSQL Vacuum

### Definition:

PostgreSQL Vacuum is a maintenance process that reclaims storage space and updates statistics by removing dead tuples (deleted or outdated rows) from tables. Unlike other databases that immediately remove deleted data, PostgreSQL uses Multi-Version Concurrency Control (MVCC) which marks rows as deleted but leaves them in place. Vacuum cleans up these "dead" rows, prevents transaction ID wraparound, and keeps the database performing efficiently.

**Key Properties:**

- Space reclamation: Removes dead tuples to free up storage
- Statistics updates: Refreshes query planner statistics for optimal performance
- Transaction ID management: Prevents transaction ID wraparound issues
- Concurrent operation: Can run alongside normal database operations
- Automatic scheduling: AUTOVACUUM runs automatically based on thresholds
- Bloat reduction: Keeps tables and indexes from growing unnecessarily large

**Core Concepts:**

- Dead tuples: Rows marked as deleted but still physically present
- MVCC (Multi-Version Concurrency Control): PostgreSQL's concurrency mechanism
- Transaction ID wraparound: Critical issue prevented by regular vacuuming
- Table bloat: Wasted space from accumulated dead tuples
- AUTOVACUUM: Background process that automatically runs vacuum
- Vacuum thresholds: Conditions that trigger automatic vacuuming

### Example:

E-commerce database experiencing performance degradation due to frequent updates and deletes

```
-- üßπ MANUAL VACUUM OPERATIONS

-- Basic vacuum - removes dead tuples, updates statistics
VACUUM users;

-- Verbose vacuum - shows detailed information
VACUUM VERBOSE users;

-- Vacuum with analyze - also updates query planner statistics
VACUUM ANALYZE users;

-- Full vacuum - rebuilds table completely (locks table)
VACUUM FULL users;

-- Check table bloat before vacuum
SELECT
  schemaname,
  tablename,
  n_dead_tup as dead_tuples,
  n_live_tup as live_tuples,
  ROUND((n_dead_tup::float / NULLIF(n_live_tup + n_dead_tup, 0)) * 100, 2) as dead_percentage
FROM pg_stat_user_tables
WHERE tablename = 'users';
```

---

day - 14

## Floyd-Warshall Algorithm

### Definition:

The Floyd-Warshall Algorithm is a dynamic programming algorithm that finds the shortest paths between all pairs of vertices in a weighted graph. It works with both positive and negative edge weights (but no negative cycles).

**Key characteristics:**

- Time complexity: O(V¬≥)
- Space complexity: O(V¬≤)
- Works on directed and undirected graphs
- Finds shortest paths between ALL pairs of nodes

### Example:

Let's use a graph with 4 nodes (A, B, C, D):

```
A ----2---- B
|           |
5           1
|           |
C ----3---- D
Step 1: Create distance matrix (‚àû means no direct path)

     A  B  C  D
A [  0  2  5  ‚àû ]
B [  ‚àû  0  ‚àû  1 ]
C [  ‚àû  ‚àû  0  3 ]
D [  ‚àû  ‚àû  ‚àû  0 ]
Step 2: For each intermediate node k, update shortest paths

k = A: Check if going through A gives shorter path
k = B: Check if going through B gives shorter path
k = C: Check if going through C gives shorter path
k = D: Check if going through D gives shorter path
Final Result:

     A  B  C  D
A [  0  2  5  3 ]  ‚Üê A‚ÜíD: A‚ÜíB‚ÜíD = 2+1 = 3
B [  ‚àû  0  ‚àû  1 ]
C [  ‚àû  ‚àû  0  3 ]
D [  ‚àû  ‚àû  ‚àû  0 ]
```

---

day - 17

## Split-Brain Resolution

### Definition:

Split-Brain Resolution is a mechanism in distributed systems that prevents data corruption and inconsistency when network partitions occur. A "split-brain" happens when cluster nodes lose communication with each other but continue operating independently, potentially creating conflicting data states.

**Key characteristics:**

- Prevents multiple nodes from acting as "master" simultaneously
- Maintains data consistency during network failures
- Uses techniques like quorum, fencing, or witness nodes

### Example:

Banking System

```
Imagine a bank with two data centers (Data Center A and Data Center B) that synchronize customer accounts:

Data Center A ‚Üê--X--‚Üí Data Center B
(Customer Balance: $$1000)   (Customer Balance: $$1000)
A network partition occurs - they can't communicate, but both stay active.

Without Split-Brain Resolution:

A customer withdraws 500 from Data Center A's ATM, and the same customer withdraws 600 from Data Center B's ATM.

Problem: The customer has withdrawn a total of 1100 from the account, resulting in a balance of 1100 when it should be 0! üí∏
```

---

day - 18

## The DSPM Paradox

### Definition:

The DSPM (Data Security Posture Management) Paradox refers to the counterintuitive situation where organizations implement more data security tools and controls to protect their sensitive data, but this increased complexity actually makes it harder to know where their data is, how it's being used, and whether it's truly secure.

**Key characteristics:**

- More security tools = Less visibility
- Increased compliance efforts = Decreased data awareness
- Multiple protection layers = Fragmented data knowledge
- Security sprawl creates blind spots

### Example:

Company's Journey: From Simple to Complex

```
Year 1 - Simple Setup:

üìä Customer Database ‚Üí üîí Basic Firewall
- Data location: Clear - one database
- Security status: Visible - can easily check who accesses what
- Risk level: Known and manageable

Year 5 - "Secure" Setup:

üìä Database ‚Üí üîí Firewall ‚Üí üõ°Ô∏è DLP ‚Üí üîê Encryption ‚Üí ‚òÅÔ∏è Cloud Storage
                     ‚Üì
            üîç SIEM ‚Üí üìã CASB ‚Üí üö® EDR ‚Üí üìä Analytics
The Paradox Emerges:

- More security tools: 8+ different security products
- Less visibility: Data scattered across multiple systems
- Unknown data locations: "Where is our customer PII actually stored?"
- Compliance confusion: Different tools report different things
- Security theater: Lots of alerts, but unclear if data is actually safer
```

---
